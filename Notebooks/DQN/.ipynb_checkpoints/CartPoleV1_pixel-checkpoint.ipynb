{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN demoed with OpenAI Gym\n",
    "\n",
    "+ DQN Configuration: \n",
    "    + Experience Replay + Target Network\n",
    "    + Architecture: CNN + FC\n",
    " \n",
    " \n",
    "+ Gym Environment: `CartPoleV1`\n",
    "    + Raw pixel input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v1').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Experience Replay Memory\n",
    "\n",
    "+ `Transition` - a named tuple representing a single transition in our environment. It maps essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "+ `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a `.sample()` method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "- Takes in 4 consecutive frames stacked together (#input channels = 4), and outputs a vector of size 2, representing $Q(s, left)$ and $Q(s, right)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, w):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so comput it.\n",
    "        def conv2d_output_size(size, kernel_size=3, stride=1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        # Takes 3 consecutive images - #channel = 9\n",
    "        self.conv1 = nn.Conv2d(9, 32, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # Size Immediately after convolution\n",
    "        conv1w = conv2d_output_size(w, kernel_size=3, stride=1)\n",
    "        conv1h = conv2d_output_size(h, kernel_size=3, stride=1)\n",
    "        # Size after max pooling\n",
    "        conv1w = conv2d_output_size(conv1w, kernel_size=2, stride=2)\n",
    "        conv1h = conv2d_output_size(conv1h, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # Similarly\n",
    "        conv2w = conv2d_output_size(conv1w, kernel_size=3, stride=1)\n",
    "        conv2h = conv2d_output_size(conv1h, kernel_size=3, stride=1)\n",
    "        conv2w = conv2d_output_size(conv2w, kernel_size=2, stride=2)\n",
    "        conv2h = conv2d_output_size(conv2h, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        conv3w = conv2d_output_size(conv2w, kernel_size=5, stride=2)\n",
    "        conv3h = conv2d_output_size(conv2h, kernel_size=5, stride=2)\n",
    "        conv3w = conv2d_output_size(conv3w, kernel_size=2, stride=2)\n",
    "        conv3h = conv2d_output_size(conv3h, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        linear_input_size = conv3w * conv3h * 32\n",
    "        self.FC1 = nn.Linear(linear_input_size, 32)\n",
    "        self.FC2 = nn.Linear(32, 16)\n",
    "        self.FC3 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.bn1(self.elu(self.conv1(x))))\n",
    "        x = self.maxpool(self.bn2(self.elu(self.conv2(x))))\n",
    "        x = self.maxpool(self.bn3(self.elu(self.conv3(x))))\n",
    "        x = torch.flatten(x, start_dim=1) # Flatten\n",
    "        x = self.elu(self.FC1(x))\n",
    "        x = self.elu(self.FC2(x))\n",
    "        x = self.elu(self.FC3(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(), T.Resize(50), T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen(env):\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[int(screen_height * 0.25):int(screen_height * 0.55), :, :]\n",
    "    # Transpose to size *C * H * W)\n",
    "    screen.transpose((2, 0, 1))\n",
    "    # Resize to add a batch dimension (B * C * H * W) and send to device\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACUCAYAAACDUNJlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8kmFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow((get_screen(env).cpu().squeeze(0).numpy() * 255).astype(dtype=np.uint8).transpose(1, 2, 0))\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "gamma = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "target_update = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_screen = get_screen(env)\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (FC1): Linear(in_features=576, out_features=32, bias=True)\n",
       "  (FC2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (FC3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(screen_height, screen_width).to(device)\n",
    "target_net = DQN(screen_height, screen_width).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, net):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY) # Annealing\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest value for column of each row. \n",
    "            # second column on max result is index of where max element was found, \n",
    "            # so we pick action with the larger expected reward\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_for_evaluation(state, net):\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) > 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(batch_size, memory, policy_net, target_net):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements \n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s : s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "    # These are the actions which would've been taken for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    # Expected values of actions for non_final_next_states are computed based on the \"older\" target_net;\n",
    "    # selecting their best reward with max(1)[0]. \n",
    "    # This is merged based on the mask, such that we'll have either the expected state value or 0 \n",
    "    # in case the state was final\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "ckpt_dir = \"CartPoleV1_pixel_checkpoints/\"\n",
    "save_ckpt_interval = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(file_dir, policy_net, target_net, optimizer, i_episode, episode_durations):\n",
    "    save_dict = {\"policy_net\": policy_net.state_dict(),\n",
    "                 \"target_net\": target_net.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict(),\n",
    "                 \"i_episode\": i_episode,\n",
    "                 \"episode_durations\": episode_durations}\n",
    "    # Create the directory if not exist\n",
    "    if not os.path.isdir(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "    torch.save(save_dict, os.path.join(file_dir, \"ckpt_eps%d.pt\" % i_episode))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(file_dir, i_episode):\n",
    "    checkpoint = torch.load(os.path.join(file_dir, \"ckpt_eps%d.pt\" % i_episode))\n",
    "    \n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "    policy_net.train()\n",
    "    \n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters())\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    i_episode = checkpoint[\"i_episode\"]\n",
    "    episode_durations = checkpoint[\"episode_durations\"]\n",
    "    \n",
    "    return policy_net, target_net, optimizer, i_episode, episode_durations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.train()\n",
    "\n",
    "while True:\n",
    "    # Every save_ckpt_interval, Check if there is any checkpoint. \n",
    "    # If there is, load checkpoint and continue training\n",
    "    # Need to specify the i_episode of the checkpoint intended to load\n",
    "    if i_episode % save_ckpt_interval == 0 and os.path.isfile(os.path.join(ckpt_dir, \"ckpt_eps%d.pt\" % i_episode)):\n",
    "        policy_net, target_net, optimizer, i_episode, episode_durations = load_checkpoint(ckpt_dir, i_episode)\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    screen_0 = get_screen(env)\n",
    "    screen_1 = get_screen(env)\n",
    "    screen_2 = get_screen(env)\n",
    "    state = torch.cat([screen_0, screen_1, screen_2], dim=1)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, policy_net)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Observe new state\n",
    "        screen_0 = screen_1\n",
    "        screen_1 = screen_2\n",
    "        screen_2 = get_screen(env)\n",
    "        \n",
    "        if not done:\n",
    "            next_state = torch.cat([screen_0, screen_1, screen_2], dim=1)\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model(batch_size, memory, policy_net, target_net)\n",
    "        if done:\n",
    "            # Save and print episode stats\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            print(\"Episode: %d Cumulative Rewards: %d\" % (i_episode + 1, t + 1))\n",
    "            break\n",
    "            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    # Every save_ckpt_interval, save a checkpoint according to current i_episode.\n",
    "    # Note that we use i_episode + 1\n",
    "    if (i_episode + 1) % save_ckpt_interval == 0:\n",
    "        save_checkpoint(ckpt_dir, policy_net, target_net, optimizer, i_episode + 1, episode_durations)\n",
    "    \n",
    "    \n",
    "    i_episode += 1\n",
    "        \n",
    "print(\"Complete\")\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Rewards: 62\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "policy_net.eval()\n",
    "screen_0 = get_screen(env)\n",
    "screen_1 = get_screen(env)\n",
    "screen_2 = get_screen(env)\n",
    "state = torch.cat([screen_0, screen_1, screen_2], dim=1)\n",
    "\n",
    "for t in count():\n",
    "    # Select and perform an action\n",
    "    action = select_action(state, policy_net)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    # Observe new state\n",
    "    screen_0 = screen_1\n",
    "    screen_1 = screen_2\n",
    "    screen_2 = get_screen(env)\n",
    "\n",
    "    if not done:\n",
    "        next_state = torch.cat([screen_0, screen_1, screen_2], dim=1)\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    # Store the transition in memory\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print(\"Cumulative Rewards: %d\" % (t + 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
