{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved DQN demoed with OpenAI Gym\n",
    "\n",
    "- DQN Configuration:\n",
    "    - Prioritized Experience Replay (PER)\n",
    "    - Double DQN (implemented)\n",
    "    - Dueling DQN (implemented)\n",
    "    - Noisy Nets\n",
    "- Network Architecture:\n",
    "    - CNN * 3 + FC * 3\n",
    "- Gym Environment:  `CartPoleV1`\n",
    "    + Numerical observations input\n",
    "    \n",
    "    `    Num  Observation               Min         Max`\n",
    "    \n",
    "    `    0    Cart Position             -4.8        4.8`\n",
    "        \n",
    "    `    1    Cart Velocity             -Inf        Inf`\n",
    "        \n",
    "    `    2    Pole Angle                -24°        24°`\n",
    "        \n",
    "    `    3    Pole Velocity At Tip      -Inf        Inf`\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v1').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Experience Replay Memory\n",
    "\n",
    "+ `Transition` - a named tuple representing a single transition in our environment. It maps essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "+ `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a `.sample()` method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "- Takes in 4 numerical values from observation `(Cart Position, Cart Velocity, Pole Angle, Pole Velocity At Tip)`, and outputs a vector of size 2, representing $Q(s, left)$ and $Q(s, right)$ \n",
    "- Dueling DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN Implementation\n",
    "# 2 FC Layers to calculate Q(s, a):\n",
    "#   Value stream calculates V(s)\n",
    "#   Advantage stream calculates A(s, a) for each action a\n",
    "# Aggregation layer: Q(s, a) = V(s) + (A(s, a) - mean value across actions(A(s, )))\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # 3 FC layers to encode observations\n",
    "        self.FC1 = nn.Linear(4, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.FC2 = nn.Linear(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.FC3 = nn.Linear(64, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Dueling DQN: two streams \n",
    "        # Value stream: output dimension - 1\n",
    "        self.VFC1 = nn.Linear(64, 32)\n",
    "        self.vbn1 = nn.BatchNorm1d(32)\n",
    "        self.VFC2 = nn.Linear(32, 16)\n",
    "        self.vbn2 = nn.BatchNorm1d(16)\n",
    "        self.VFC3 = nn.Linear(16, 1)\n",
    "        # Action stream: output dimension - 2\n",
    "        self.AFC1 = nn.Linear(64, 32)\n",
    "        self.abn1 = nn.BatchNorm1d(32)\n",
    "        self.AFC2 = nn.Linear(32, 16)\n",
    "        self.abn2 = nn.BatchNorm1d(16)\n",
    "        self.AFC3 = nn.Linear(16, 2)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.elu(self.FC1(x)))\n",
    "        x = self.bn2(self.elu(self.FC2(x)))\n",
    "        x = self.bn3(self.elu(self.FC3(x)))\n",
    "        #print(\"x shape: \", x.shape)\n",
    "        \n",
    "        # Value stream: \n",
    "        v = self.vbn1(self.elu(self.VFC1(x)))\n",
    "        v = self.vbn2(self.elu(self.VFC2(v)))\n",
    "        v = self.elu(self.VFC3(v))\n",
    "        #print(\"v shape:\", v.shape)\n",
    "        \n",
    "        # Action sream:\n",
    "        a = self.abn1(self.elu(self.AFC1(x)))\n",
    "        a = self.abn2(self.elu(self.AFC2(a)))\n",
    "        a = self.elu(self.AFC3(a))\n",
    "        #print(\"a shape:\", a.shape)\n",
    "        \n",
    "        # Aggregate:\n",
    "        #print(\"a mean shape: \", a.mean(dim=-1).shape)\n",
    "        #print(\"a substracted shape:\", (a - a.mean(dim=-1, keepdim=True)).shape)\n",
    "        q = v + (a - a.mean(dim=-1, keepdim=True))\n",
    "        #print(\"q shape:\", q.shape)\n",
    "        \n",
    "        return q\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "gamma = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "target_update = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (FC1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (FC2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (FC3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (VFC1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (vbn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (VFC2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (vbn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (VFC3): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (AFC1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (abn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (AFC2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (abn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (AFC3): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, net):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY) # Annealing\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest value for column of each row. \n",
    "            # second column on max result is index of where max element was found, \n",
    "            # so we pick action with the larger expected reward\n",
    "            \n",
    "            # DOUBLE DQN implementation:\n",
    "            #. we use the online policy net to greedily select the action \n",
    "            #. and the target net to estimate the Q-value\n",
    "            #. so NO CHANGE HERE\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_for_evaluation(state, net):\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) > 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(batch_size, memory, policy_net, target_net):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements \n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s : s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "    # These are the actions which would've been taken for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    # Expected values of actions for non_final_next_states are computed\n",
    "    # This is merged based on the mask, such that we'll have either the expected state value or 0 \n",
    "    # in case the state was final\n",
    "    \n",
    "    # DOUBLE DQN implementation:\n",
    "    #. we use the online policy net to greedily select the action \n",
    "    #. and the target net to estimate the Q-value\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_action_policynet_decisions = policy_net(non_final_next_states).max(1)[1]\n",
    "    non_final_next_state_targetnet_values = target_net(non_final_next_states) \\\n",
    "        .gather(1, next_action_policynet_decisions.view(-1, 1).repeat(1, 2))[:, 0]\n",
    "    next_state_values[non_final_mask] = non_final_next_state_targetnet_values.detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(file_dir, policy_net, target_net, optimizer, i_episode, episode_durations):\n",
    "    save_dict = {\"policy_net\": policy_net.state_dict(),\n",
    "                 \"target_net\": target_net.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict(),\n",
    "                 \"i_episode\": i_episode,\n",
    "                 \"episode_durations\": episode_durations}\n",
    "    # Create the directory if not exist\n",
    "    if not os.path.isdir(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "    torch.save(save_dict, os.path.join(file_dir, \"ckpt_eps%d.pt\" % i_episode))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(file_dir, i_episode):\n",
    "    checkpoint = torch.load(os.path.join(file_dir, \"ckpt_eps%d.pt\" % i_episode))\n",
    "    \n",
    "    policy_net = DQN().to(device)\n",
    "    policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "    policy_net.train()\n",
    "    \n",
    "    target_net = DQN().to(device)\n",
    "    target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters())\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    i_episode = checkpoint[\"i_episode\"]\n",
    "    episode_durations = checkpoint[\"episode_durations\"]\n",
    "    \n",
    "    return policy_net, target_net, optimizer, i_episode, episode_durations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "ckpt_dir = \"DDDQN_CartPoleV1_obs_checkpoints/\"\n",
    "save_ckpt_interval = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9D2kfGKl9ZYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2094 Cumulative Rewards: 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1179cd885b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Save and print episode stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-769e68a92992>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(batch_size, memory, policy_net, target_net)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#. and the target net to estimate the Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnext_action_policynet_decisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mnon_final_next_state_targetnet_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action_policynet_decisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mnext_state_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_final_next_state_targetnet_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/townes/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-2f4793a0765e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#print(\"x shape: \", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/townes/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/townes/anaconda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/townes/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1348\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     )\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_net.train()\n",
    "\n",
    "while True:\n",
    "    # Every save_ckpt_interval, Check if there is any checkpoint. \n",
    "    # If there is, load checkpoint and continue training\n",
    "    # Need to specify the i_episode of the checkpoint intended to load\n",
    "    if i_episode % save_ckpt_interval == 0 and os.path.isfile(os.path.join(ckpt_dir, \"ckpt_eps%d.pt\" % i_episode)):\n",
    "        policy_net, target_net, optimizer, i_episode, episode_durations = load_checkpoint(ckpt_dir, i_episode)\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    observation = env.reset()\n",
    "    current_state = torch.tensor([observation], device=device, dtype=torch.float32)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        # Turn policy_net into evaluation mode to select an action given a single state\n",
    "        policy_net.eval()\n",
    "        \n",
    "        action = select_action(current_state, policy_net)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        env.render()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if not done:\n",
    "            next_state = torch.tensor([observation], device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        # Store the transition in memory\n",
    "        memory.push(current_state, action, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        current_state = next_state\n",
    "        \n",
    "        # Turn policy_net back to training mode to optimize on a batch of experiences\n",
    "        policy_net.train()\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model(batch_size, memory, policy_net, target_net)\n",
    "        if done:\n",
    "            # Save and print episode stats\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            print(\"Episode: %d Cumulative Rewards: %d\" % (i_episode + 1, t + 1))\n",
    "            break\n",
    "            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    # Every save_ckpt_interval, save a checkpoint according to current i_episode.\n",
    "    # Note that we use i_episode + 1\n",
    "    if (i_episode + 1) % save_ckpt_interval == 0:\n",
    "        save_checkpoint(ckpt_dir, policy_net, target_net, optimizer, i_episode + 1, episode_durations)\n",
    "    \n",
    "    \n",
    "    i_episode += 1\n",
    "        \n",
    "print(\"Complete\")\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2095 Cumulative Rewards: 45\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "\n",
    "# Initialize the environment and state\n",
    "observation = env.reset()\n",
    "current_state = torch.tensor([observation], device=device, dtype=torch.float32)\n",
    "\n",
    "for t in count():\n",
    "    # Select and perform an action\n",
    "    action = select_action(current_state, policy_net)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    env.render()\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    if not done:\n",
    "        next_state = torch.tensor([observation], device=device, dtype=torch.float32)\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    # Move to the next state\n",
    "    current_state = next_state\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode: %d Cumulative Rewards: %d\" % (i_episode + 1, t + 1))\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
